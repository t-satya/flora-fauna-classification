{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a77ebdb7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-03T05:50:49.096867Z",
     "iopub.status.busy": "2025-08-03T05:50:49.095883Z",
     "iopub.status.idle": "2025-08-03T05:50:50.320667Z",
     "shell.execute_reply": "2025-08-03T05:50:50.320016Z"
    },
    "papermill": {
     "duration": 1.232456,
     "end_time": "2025-08-03T05:50:50.322674",
     "exception": false,
     "start_time": "2025-08-03T05:50:49.090218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b492b994",
   "metadata": {
    "papermill": {
     "duration": 0.003394,
     "end_time": "2025-08-03T05:50:50.329664",
     "exception": false,
     "start_time": "2025-08-03T05:50:50.326270",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46a7beed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T05:50:50.337366Z",
     "iopub.status.busy": "2025-08-03T05:50:50.337034Z",
     "iopub.status.idle": "2025-08-03T05:51:00.358668Z",
     "shell.execute_reply": "2025-08-03T05:51:00.357662Z"
    },
    "papermill": {
     "duration": 10.027592,
     "end_time": "2025-08-03T05:51:00.360514",
     "exception": false,
     "start_time": "2025-08-03T05:50:50.332922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7a5342fead70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports necessary for training the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.utils as utils\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Few other important libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d064c7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T05:51:00.369139Z",
     "iopub.status.busy": "2025-08-03T05:51:00.368670Z",
     "iopub.status.idle": "2025-08-03T05:51:00.372689Z",
     "shell.execute_reply": "2025-08-03T05:51:00.371858Z"
    },
    "papermill": {
     "duration": 0.010063,
     "end_time": "2025-08-03T05:51:00.374428",
     "exception": false,
     "start_time": "2025-08-03T05:51:00.364365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ef0e6e",
   "metadata": {
    "papermill": {
     "duration": 0.003135,
     "end_time": "2025-08-03T05:51:00.383630",
     "exception": false,
     "start_time": "2025-08-03T05:51:00.380495",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83cc1023",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T05:51:00.391599Z",
     "iopub.status.busy": "2025-08-03T05:51:00.391336Z",
     "iopub.status.idle": "2025-08-03T05:51:00.397551Z",
     "shell.execute_reply": "2025-08-03T05:51:00.396890Z"
    },
    "papermill": {
     "duration": 0.012251,
     "end_time": "2025-08-03T05:51:00.399204",
     "exception": false,
     "start_time": "2025-08-03T05:51:00.386953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Resize(232),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToImage(),  # converts from HWC np to CHW Tensor\n",
    "    transforms.ToDtype(torch.float32, scale=True),  # uint8 [0,255] -> float32 [0.0, 1.0]\n",
    "    transforms.Resize(256),  \n",
    "    transforms.CenterCrop(224),  # No random cropping here\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9fc6ec",
   "metadata": {
    "papermill": {
     "duration": 0.003073,
     "end_time": "2025-08-03T05:51:00.405909",
     "exception": false,
     "start_time": "2025-08-03T05:51:00.402836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "412e57f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T05:51:00.413801Z",
     "iopub.status.busy": "2025-08-03T05:51:00.413071Z",
     "iopub.status.idle": "2025-08-03T05:51:00.450647Z",
     "shell.execute_reply": "2025-08-03T05:51:00.449687Z"
    },
    "papermill": {
     "duration": 0.043259,
     "end_time": "2025-08-03T05:51:00.452281",
     "exception": false,
     "start_time": "2025-08-03T05:51:00.409022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path = \"/kaggle/input/deep-learning-practice-image-classification/train\"\n",
    "test_path = \"/kaggle/input/deep-learning-practice-image-classification/test\"\n",
    "\n",
    "\n",
    "from PIL import Image  # Import the Image module from Pillow\n",
    "\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, test_dir, transform=None):\n",
    "        self.test_dir = test_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(test_dir, fname) for fname in os.listdir(test_dir)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Open image and convert to RGB\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply the transform if available\n",
    "        return image, img_path  # Return the image and its path\n",
    "\n",
    "\n",
    "# Create test data\n",
    "test_data = CustomTestDataset(test_dir=test_path, transform=transform_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a86b93d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T05:51:00.460425Z",
     "iopub.status.busy": "2025-08-03T05:51:00.460184Z",
     "iopub.status.idle": "2025-08-03T05:51:06.875908Z",
     "shell.execute_reply": "2025-08-03T05:51:06.874837Z"
    },
    "papermill": {
     "duration": 6.422013,
     "end_time": "2025-08-03T05:51:06.877814",
     "exception": false,
     "start_time": "2025-08-03T05:51:00.455801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape:  9999\n",
      "Test Data Shape:  2000\n"
     ]
    }
   ],
   "source": [
    "# Load Training and Test datasets\n",
    "full_train_data = dsets.ImageFolder(root=train_path)\n",
    "\n",
    "# Create a validation set from the training data\n",
    "train_size_fraction = 0.9\n",
    "train_size = int(train_size_fraction*len(full_train_data))  # 90% for training\n",
    "val_size = len(full_train_data) - train_size  # 10% for validation\n",
    "train_dataset, val_dataset = random_split(full_train_data, [train_size, val_size])\n",
    "\n",
    "# Now apply transform_train and transform_test separately\n",
    "train_dataset.dataset.transform = transform_train  # augmentation\n",
    "val_dataset.dataset.transform = transform_test      # no augmentation\n",
    "\n",
    "# Print data details\n",
    "print(\"Train Data Shape: \", len(full_train_data))\n",
    "print(\"Test Data Shape: \", len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d6bcb94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T05:51:06.886282Z",
     "iopub.status.busy": "2025-08-03T05:51:06.886008Z",
     "iopub.status.idle": "2025-08-03T05:51:06.890263Z",
     "shell.execute_reply": "2025-08-03T05:51:06.889343Z"
    },
    "papermill": {
     "duration": 0.010138,
     "end_time": "2025-08-03T05:51:06.891825",
     "exception": false,
     "start_time": "2025-08-03T05:51:06.881687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_mapping {'Amphibia': 0, 'Animalia': 1, 'Arachnida': 2, 'Aves': 3, 'Fungi': 4, 'Insecta': 5, 'Mammalia': 6, 'Mollusca': 7, 'Plantae': 8, 'Reptilia': 9}\n"
     ]
    }
   ],
   "source": [
    "print(\"label_mapping\",full_train_data.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79ca13b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T05:51:06.899489Z",
     "iopub.status.busy": "2025-08-03T05:51:06.899259Z",
     "iopub.status.idle": "2025-08-03T05:51:06.905422Z",
     "shell.execute_reply": "2025-08-03T05:51:06.904411Z"
    },
    "papermill": {
     "duration": 0.012133,
     "end_time": "2025-08-03T05:51:06.907322",
     "exception": false,
     "start_time": "2025-08-03T05:51:06.895189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of samples in training dataset: 8999\n",
      "No. of samples in validation dataset: 1000\n",
      "No. of samples in test dataset: 2000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64 # Batch Size of the images\n",
    "\n",
    "# Creating dataloaders\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True,num_workers=2)\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size = batch_size, shuffle = False,num_workers=2)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = False,num_workers=2)\n",
    "\n",
    "# Printing the no. of samples in each dataset\n",
    "print(\"No. of samples in training dataset:\", len(train_loader.dataset))\n",
    "print(\"No. of samples in validation dataset:\", len(val_loader.dataset))\n",
    "print(\"No. of samples in test dataset:\", len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ca4bc2",
   "metadata": {
    "papermill": {
     "duration": 0.003355,
     "end_time": "2025-08-03T05:51:06.914117",
     "exception": false,
     "start_time": "2025-08-03T05:51:06.910762",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training and Validating Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e594dec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T05:51:06.922670Z",
     "iopub.status.busy": "2025-08-03T05:51:06.922399Z",
     "iopub.status.idle": "2025-08-03T05:51:06.935364Z",
     "shell.execute_reply": "2025-08-03T05:51:06.934729Z"
    },
    "papermill": {
     "duration": 0.018968,
     "end_time": "2025-08-03T05:51:06.936903",
     "exception": false,
     "start_time": "2025-08-03T05:51:06.917935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(preds, labels):\n",
    "    correct = (preds == labels).sum().item()\n",
    "    return correct / len(labels) * 100\n",
    "\n",
    "# Define the training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=0.0001,patience=2):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2)\n",
    "\n",
    "    best_val_f1 = 0.0  # Track the best F1 score\n",
    "    best_model_weights = None  # Store best model weights\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        all_train_preds = []\n",
    "        all_train_labels = []\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Get predictions\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_corrects += (preds == labels).sum().item()\n",
    "\n",
    "            all_train_preds.extend(preds.cpu().numpy())\n",
    "            all_train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Calculate training accuracy and F1 score\n",
    "        train_accuracy = calculate_accuracy(torch.tensor(all_train_preds), torch.tensor(all_train_labels))\n",
    "        train_f1 = f1_score(all_train_labels, all_train_preds, average='weighted')\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Train F1: {train_f1:.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        running_val_loss = 0.0\n",
    "        running_val_corrects = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                running_val_loss += loss.item()\n",
    "                running_val_corrects += (preds == labels).sum().item()\n",
    "\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Calculate validation accuracy and F1 score\n",
    "        val_accuracy = calculate_accuracy(torch.tensor(val_preds), torch.tensor(val_labels))\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "\n",
    "        avg_val_loss = running_val_loss / len(val_loader)\n",
    "        print(f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # Step the learning rate scheduler\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Save the best model based on validation F1 score\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_model_weights = model.state_dict()\n",
    "            epochs_since_improvement = 0  # Reset patience counter\n",
    "            print(f\"Saved best model with validation F1: {val_f1:.4f}\")\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "            \n",
    "        # Early stopping check\n",
    "        if epochs_since_improvement >= patience:\n",
    "            print(f\"Stopping training early at epoch {epoch+1} due to no improvement in validation F1.\")\n",
    "            break\n",
    "\n",
    "    # Load best model weights\n",
    "    if best_model_weights:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "\n",
    "    print(f\"Training Complete. Best Validation F1 Score: {best_val_f1:.4f}\")\n",
    "    return model, best_val_f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7258a95d",
   "metadata": {
    "papermill": {
     "duration": 0.003048,
     "end_time": "2025-08-03T05:51:06.943381",
     "exception": false,
     "start_time": "2025-08-03T05:51:06.940333",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluating Test and Saving Predictions to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c95609c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T05:51:06.950928Z",
     "iopub.status.busy": "2025-08-03T05:51:06.950649Z",
     "iopub.status.idle": "2025-08-03T05:51:06.956753Z",
     "shell.execute_reply": "2025-08-03T05:51:06.955936Z"
    },
    "papermill": {
     "duration": 0.011796,
     "end_time": "2025-08-03T05:51:06.958377",
     "exception": false,
     "start_time": "2025-08-03T05:51:06.946581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_and_save_predictions(model, test_loader, output_csv='21F1000641.csv'):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)  # Ensure model is on the correct device\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    predictions = []\n",
    "    image_ids = []\n",
    "    \n",
    "    # Disable gradient computation for inference\n",
    "    with torch.no_grad():\n",
    "        for inputs, paths in tqdm(test_loader):  # Assuming test_loader provides file paths\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            # Extract Image_IDs (file names without extension)\n",
    "            for path, pred in zip(paths, preds):\n",
    "                image_id = os.path.splitext(os.path.basename(path))[0]  # Remove extension from the filename\n",
    "                predictions.append(pred.item())  # Add the predicted label\n",
    "                image_ids.append(image_id)  # Add the image ID\n",
    "\n",
    "    # Create a DataFrame with Image_ID and predicted labels\n",
    "    submission_df = pd.DataFrame({\n",
    "        'Image_ID': image_ids,\n",
    "        'Label': predictions\n",
    "    })\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    output_csv_path = f\"/kaggle/working/{output_csv}\"\n",
    "    submission_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cb9671c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T05:51:06.965848Z",
     "iopub.status.busy": "2025-08-03T05:51:06.965591Z",
     "iopub.status.idle": "2025-08-03T06:18:17.939423Z",
     "shell.execute_reply": "2025-08-03T06:18:17.938234Z"
    },
    "papermill": {
     "duration": 1631.015269,
     "end_time": "2025-08-03T06:18:17.977053",
     "exception": false,
     "start_time": "2025-08-03T05:51:06.961784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to /root/.cache/torch/hub/checkpoints/convnext_tiny-983f1562.pth\n",
      "100%|██████████| 109M/109M [00:00<00:00, 197MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ConvNeXt_Tiny:\n",
      "Epoch 1/30\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [05:16<00:00,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8341, Train Accuracy: 76.65%, Train F1: 0.7668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4011, Val Accuracy: 88.90%, Val F1: 0.8897\n",
      "Saved best model with validation F1: 0.8897\n",
      "Epoch 2/30\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [05:14<00:00,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2223, Train Accuracy: 93.99%, Train F1: 0.9400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3416, Val Accuracy: 90.10%, Val F1: 0.9011\n",
      "Saved best model with validation F1: 0.9011\n",
      "Epoch 3/30\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [05:14<00:00,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0723, Train Accuracy: 98.28%, Train F1: 0.9828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3396, Val Accuracy: 90.90%, Val F1: 0.9095\n",
      "Saved best model with validation F1: 0.9095\n",
      "Epoch 4/30\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [05:15<00:00,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0312, Train Accuracy: 99.41%, Train F1: 0.9941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4255, Val Accuracy: 89.90%, Val F1: 0.8988\n",
      "Epoch 5/30\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [05:15<00:00,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0264, Train Accuracy: 99.41%, Train F1: 0.9941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4397, Val Accuracy: 89.50%, Val F1: 0.8952\n",
      "Stopping training early at epoch 5 due to no improvement in validation F1.\n",
      "Training Complete. Best Validation F1 Score: 0.9095\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import convnext_tiny, ConvNeXt_Tiny_Weights\n",
    "\n",
    "\n",
    "# Load pretrained ConvNeXt Tiny\n",
    "weights = ConvNeXt_Tiny_Weights.DEFAULT\n",
    "model = convnext_tiny(weights=weights)\n",
    "\n",
    "# Freeze the feature extractor layers\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = True\n",
    "       \n",
    "\n",
    "# Replace the classifier for your number of classes\n",
    "model.classifier[2] = nn.Linear(model.classifier[2].in_features, 10)\n",
    "\n",
    "\n",
    "# Training the model\n",
    "print(\"Training ConvNeXt_Tiny:\")\n",
    "trained_model, best_f1 = train_model(model, train_loader, val_loader, num_epochs=30, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5614521",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T06:18:18.092946Z",
     "iopub.status.busy": "2025-08-03T06:18:18.092537Z",
     "iopub.status.idle": "2025-08-03T06:18:45.057949Z",
     "shell.execute_reply": "2025-08-03T06:18:45.056795Z"
    },
    "papermill": {
     "duration": 27.003606,
     "end_time": "2025-08-03T06:18:45.059685",
     "exception": false,
     "start_time": "2025-08-03T06:18:18.056079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:26<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to /kaggle/working/21F1000641.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_save_predictions(trained_model, test_loader, output_csv='21F1000641.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a929dbba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T06:18:45.135488Z",
     "iopub.status.busy": "2025-08-03T06:18:45.135186Z",
     "iopub.status.idle": "2025-08-03T06:18:45.145255Z",
     "shell.execute_reply": "2025-08-03T06:18:45.144494Z"
    },
    "papermill": {
     "duration": 0.049857,
     "end_time": "2025-08-03T06:18:45.146910",
     "exception": false,
     "start_time": "2025-08-03T06:18:45.097053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "output_df = pd.read_csv(\"/kaggle/working/21F1000641.csv\")\n",
    "print(len(output_df))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9549185,
     "sourceId": 84763,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1682.838033,
   "end_time": "2025-08-03T06:18:48.264882",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-03T05:50:45.426849",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
